import numpy as np
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical
import time
import sys
from imblearn.over_sampling import SMOTE
import h5py
from datetime import datetime
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Flatten, Activation
from keras.models import Sequential
from sklearn.feature_extraction.text import CountVectorizer
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
from keras.callbacks import EarlyStopping

import os
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]="1"  # specify which GPU(s) to be used

import tensorflow as tf
if tf.test.gpu_device_name():
    print('    Default GPU Device: {}'.format(tf.test.gpu_device_name()))
else:
    print("    Please install GPU version of TF")
    
modelName = 'V6_620k' 

def getData(directory, filename, key):
    t = h5py.File("./CodeVectors/"+directory+"/"+filename, 'r')
    tdata = t[key]
    return tdata

def getVulnSmote(X, y):
    X_temp = []
    y_temp = []
    i = 0
    for x in X:
        if(y[i] == 1):
            X_temp.append(x)
            y_temp.append(y[i])
        i+=1
    return np.asarray(X_temp), np.asarray(y_temp)

print("{} Starting process... ".format(datetime.now()))
# All positives
X_vuln = getData("Vuln", "X_vuln", "X_0")
y_vuln = getData("Vuln", "y_vuln", "y_0")
print("  {} Read positive files".format(datetime.now()))

# Negatives
X_non_vuln = np.concatenate((getData("NonVuln", "X_0", "X_0"), 
                             getData("NonVuln", "X_1", "X_1"),
                             getData("NonVuln", "X_2", "X_2"),
                             getData("NonVuln", "X_3", "X_3"),
                             getData("NonVuln", "X_4", "X_4"), 
                             getData("NonVuln", "X_4", "X_4")))[:200000]

y_non_vuln = np.concatenate((getData("NonVuln", "y_0", "y_0"), 
                             getData("NonVuln", "y_1", "y_1"),
                             getData("NonVuln", "y_2", "y_2"),
                             getData("NonVuln", "y_3", "y_3"),
                             getData("NonVuln", "y_4", "y_4"), 
                             getData("NonVuln", "y_4", "y_4")))[:200000]
print("  {} Read negative files".format(datetime.now()))

# FPs
X_FP = getData("FP", "X_FP_train", "X_0")[:20000]
y_FP = getData("FP", "y_FP_train", "y_0")[:20000]
print("  {} Read FP files".format(datetime.now()))

# Positives 
X_SMOTE = np.concatenate((getData("Vuln", "X_smote_1", "X_smote_1"), 
                             getData("Vuln", "X_smote_2", "X_smote_2"),
                             getData("Vuln", "X_smote_3", "X_smote_3"),
                             getData("Vuln", "X_smote_4", "X_smote_4")))

y_SMOTE = np.concatenate((getData("Vuln", "y_smote_1", "y_smote_1"), 
                             getData("Vuln", "y_smote_2", "y_smote_2"),
                             getData("Vuln", "y_smote_3", "y_smote_3"),
                             getData("Vuln", "y_smote_4", "y_smote_4")))
print("  {} Read SMOTE files".format(datetime.now()))

# Duplicate vulnerable contracts in SMOTE by 2
X_SMOTE_vuln, y_SMOTE_vuln = getVulnSmote(X_SMOTE, y_SMOTE)
X_SMOTE = np.concatenate((X_SMOTE_vuln, X_SMOTE_vuln))
y_SMOTE = np.expand_dims(np.concatenate((y_SMOTE_vuln, y_SMOTE_vuln)), axis=1)
print("  ", X_SMOTE.shape, y_SMOTE.shape)
print("  {} Duplicated SMOTE".format(datetime.now()))

# Non-vuln training with FP
X_train_nonVuln = np.concatenate((X_non_vuln, X_FP), axis=0)
y_train_nonVuln = np.concatenate((y_non_vuln, y_FP), axis=0)
print("  ", X_train_nonVuln.shape, y_train_nonVuln.shape) # 200000,1600,150  200000,1

#331 
del X_SMOTE_vuln
del y_SMOTE_vuln
del X_non_vuln
del X_FP
del y_non_vuln
del y_FP
#205

X_train = np.concatenate((X_train_nonVuln, X_SMOTE), axis=0)
y_train = np.concatenate((y_train_nonVuln, y_SMOTE), axis=0)
print("  {} Concatenated X_train and y_train".format(datetime.now()))
print("# Train: ", X_train.shape, y_train.shape) # 400000,1600,150  400000,1

#442
del X_SMOTE
del y_SMOTE
del X_train_nonVuln
del y_train_nonVuln
#260

# Shuffle train because we are doing batching 
from sklearn.utils import shuffle
X_train, y_train = shuffle(X_train, y_train, random_state=42)


X_non_vuln = np.concatenate((getData("NonVuln", "X_7", "X_7"), 
                             getData("NonVuln", "X_8", "X_8"),
                             getData("NonVuln", "X_9", "X_9"),
                             getData("NonVuln", "X_10", "X_10")))[:98599]
y_non_vuln = np.concatenate((getData("NonVuln", "y_7", "y_7"), 
                             getData("NonVuln", "y_8", "y_8"),
                             getData("NonVuln", "y_9", "y_9"),
                             getData("NonVuln", "y_10", "y_10")))[:98599]
print("  ", X_non_vuln.shape, y_non_vuln.shape) # 98672,1600,150  98672,1
print("  Read X_non_vuln and y_non_vuln", datetime.now())

X_FP = getData("FP", "X_FP_val", "X_0")
y_FP = getData("FP", "y_FP_val", "y_0")
print("  Read FPs files (Validation) ", datetime.now())

X_val_vuln = X_vuln[5530:6858]
y_val_vuln = y_vuln[5530:6858]
print("  ", X_val_vuln.shape, y_val_vuln.shape) # 1328,1600,150  1328,1
print("  Read Vuln files (Validation) ", datetime.now())

X_val = np.concatenate((X_non_vuln, X_val_vuln, X_FP), axis=0)
y_val = np.concatenate((y_non_vuln, y_val_vuln, y_FP), axis=0)
print("  Concatenated validation files ", datetime.now())
print("# Validation: ", X_val.shape, y_val.shape) # 100000,1600,150  100000,1

# 363
del X_non_vuln
del X_val_vuln
del X_FP
del y_non_vuln
del y_val_vuln
del y_FP
# 308

# Shuffle validation set too
X_val, y_val = shuffle(X_val, y_val, random_state=42)


X_non_vuln = np.concatenate((getData("NonVuln", "X_11", "X_11"), 
                             getData("NonVuln", "X_12", "X_12"),
                             getData("NonVuln", "X_13", "X_13"),
                             getData("NonVuln", "X_14", "X_14")))[:118218]
y_non_vuln = np.concatenate((getData("NonVuln", "y_11", "y_11"), 
                             getData("NonVuln", "y_12", "y_12"),
                             getData("NonVuln", "y_13", "y_13"),
                             getData("NonVuln", "y_14", "y_14")))[:118218]
print("  Read X_non_vuln and y_non_vuln", datetime.now())
print("  ", X_non_vuln.shape, y_non_vuln.shape) # 118218,1600,150  118218,1

X_test_vuln = X_vuln[6858:]
y_test_vuln = y_vuln[6858:]
print("  Read Vuln files (Validation) ", datetime.now())
print("  ", X_test_vuln.shape, y_test_vuln.shape) # 1782,1600,150  1782,1

X_test = np.concatenate((X_non_vuln, X_test_vuln), axis=0)
y_test = np.concatenate((y_non_vuln, y_test_vuln), axis=0)
print("  Concatenated test files ", datetime.now())
print("# Test: ", X_test.shape, y_test.shape) # 120000,1600,150  120000,1

del X_non_vuln
del X_test_vuln
del y_non_vuln
del y_test_vuln
# 369 

import keras
import pickle
keras.backend.clear_session()

print("  Convert format for training ")
y_train_cat = to_categorical(y_train, num_classes=2)
y_val_cat = to_categorical(y_val, num_classes=2)
y_test_cat = to_categorical(y_test, num_classes=2)

print("  ",X_train.shape, y_train_cat.shape, X_val.shape, y_val_cat.shape, X_test.shape, y_test_cat.shape)

print("# Preparing model")
model = Sequential()
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, input_shape=(1600,150), return_sequences=True))
model.add(Dense(64))
model.add(LSTM(64))
model.add(Dense(2, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
print("  ",model.summary())

history = []

def train(X_train, y_train_cat, X_val, y_val_cat, batchNo):
    start_time = time.time()
    history.append(model.fit(X_train, y_train_cat, epochs=50, batch_size=256, validation_split=0.0, validation_data=(X_val, y_val_cat),callbacks=[EarlyStopping(monitor='loss',patience=7, min_delta=0.0001)]))

    end_time = time.time()
    print('  Time taken for training: ', end_time-start_time)

    print("  Saving model: "+str(batchNo))
    model.save('./SavedModels/'+modelName+'_batch'+str(batchNo)+'.h5')


noOfBatches = 10
sizeOfTrain = 400000/noOfBatches
sizeOfVal = 100000/noOfBatches

for x in range(0, noOfBatches):
    a = int(x*sizeOfTrain)
    b = int(x*sizeOfTrain+sizeOfTrain)
    c = int(x*sizeOfVal)
    d = int(x*sizeOfVal+sizeOfVal)
    train(X_train[a:b], y_train_cat[a:b], X_val[c:d], y_val_cat[c:d], x)

print("  Saving history")
with open('./PickleJar/' + 'train' + modelName, 'wb') as file_pi:
    pickle.dump(history, file_pi) 
    
print("# Evaluation (Main)")
accr = model.evaluate(X_test, y_test_cat)
print('  Test set\n  Loss: {:0.4f}\n  Accuracy: {:0.4f}'.format(accr[0],accr[1]))

# To calculate precision and recall
y_pred = model.predict_classes(X_test, batch_size=32, verbose=0)
ytest_true = y_test

from sklearn.metrics import average_precision_score

# Compute the average precision score
average_precision = average_precision_score(ytest_true, y_pred)
print('  Average Precision Score: {:0.4f}\n'.format(average_precision))

# Compute the recall
from sklearn.metrics import precision_recall_curve
from sklearn.utils.fixes import signature

precision, recall, _ = precision_recall_curve(ytest_true, y_pred)
print('  Recall Score: {:0.4f}\n'.format(recall[1]))

from sklearn.metrics import precision_score, \
    recall_score, confusion_matrix, classification_report, \
    accuracy_score, f1_score

print('  Accuracy:', accuracy_score(ytest_true, y_pred))
print('  Recall:', recall_score(ytest_true, y_pred))
print('  Precision:', precision_score(ytest_true, y_pred))
print('  F1 score:', f1_score(ytest_true, y_pred))
print('\n  clasification report:\n', classification_report(ytest_true, y_pred))
print('\n  confusion matrix:\n',confusion_matrix(ytest_true, y_pred))


# All FPs 
X_greedy_test = getData("FP", "X_greedy_test", "X_0")
y_greedy_test = getData("FP", "y_greedy_test", "y_0")

X_suicidal_test = getData("FP", "X_suicidal_test", "X_0")
y_suicidal_test = getData("FP", "y_suicidal_test", "y_0")

X_leak_test = getData("FP", "X_leak_test", "X_0")
y_leak_test = getData("FP", "y_leak_test", "y_0")

y_FP_leak_test = to_categorical(y_leak_test, num_classes=2)
y_FP_suicidal_test = to_categorical(y_suicidal_test, num_classes=2)
y_FP_greedy_test = to_categorical(y_greedy_test, num_classes=2)
print("  Evaluation (FPs) ")
leakFP_accr = model.evaluate(X_leak_test,y_FP_leak_test)
print('  Leak test set\n  Loss: {:0.4f}\n  Accuracy: {:0.4f}'.format(leakFP_accr[0],leakFP_accr[1]))

suicidalFP_accr = model.evaluate(X_suicidal_test,y_FP_suicidal_test)
print('  Suicidal test set\n  Loss: {:0.4f}\n  Accuracy: {:0.4f}'.format(suicidalFP_accr[0],suicidalFP_accr[1]))

greedyFP_accr = model.evaluate(X_greedy_test,y_FP_greedy_test)
print('  Greedy test set\n  Loss: {:0.4f}\n  Accuracy: {:0.4f}'.format(greedyFP_accr[0],greedyFP_accr[1]))
























